{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN for Anime Faces\n",
    "Project for CSE 512 Machine Learning   \n",
    "Tianchi Mo 112281322  \n",
    "Stony Brook University, Department of Computer Science   \n",
    "Fall 2019  \n",
    "\n",
    "The goal of this project is utilizing Generative Adversarial Networks (GAN) to draw non-existing anime faces. To achieve this, I try 3 techniques:\n",
    "- DCGAN (on my desktop)\n",
    "- W-GAN (on my desktop)\n",
    "- Transfer learning based on Progressive Growing of GAN (PGGAN) (on AWS)\n",
    "\n",
    "## Dataset \n",
    "I use https://github.com/Mckinsey666/Anime-Face-Dataset. After cleaning it contains 57238 faces. I resize all of them 64x64. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim  \n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torchvision.utils as vutils\n",
    "from random import randint\n",
    "from IPython.display import clear_output\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DCGAN, I define the generator network as:\n",
    "- ConvTranspose2d (take in 100-channel 1x1 noise, out channel = 64x8, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "- Batch norm \n",
    "- ReLU\n",
    "- ConvTranspose2d (in channel = 64x8, out channel = 64x4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "- Batch norm \n",
    "- ReLU\n",
    "- ConvTranspose2d (in channel = 64x4, out channel = 64x2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "- Batch norm \n",
    "- ReLU\n",
    "- ConvTranspose2d (in channel = 64x2, out channel = 64, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "- Batch norm \n",
    "- ReLU\n",
    "- Output layer: ConvTranspose2d (in channel = 64, out channel = 3, kernel_size=5, stride=3, padding=1, bias=False) + an nn.Tanh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetG(nn.Module):\n",
    "    def __init__(self, ngf, nz):\n",
    "        super(NetG, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "     \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf, 3, 5, 3, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I define the discriminator as:\n",
    "- Conv2d (in channel = 3, out channel = 64, kernel_size=5, stride=3, padding=1, bias=False),\n",
    "- Batch norm \n",
    "- Leaky ReLU\n",
    "- Conv2d (in channel = 64, out channel = 64x2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "- Batch norm \n",
    "- Leaky ReLU\n",
    "- Conv2d (in channel = 64x2, out channel = 64x4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "- Batch norm \n",
    "- Leaky ReLU\n",
    "- Conv2d (in channel = 64x4, out channel = 64x8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "- Batch norm \n",
    "- Leaky ReLU\n",
    "- Classifier: Conv2d (in channel = 64 x 8, out channel=1, kernel_size=4, stride=1, padding=0, bias=False) + an nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NetD(nn.Module):\n",
    "    def __init__(self, ndf):\n",
    "        super(NetD, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, kernel_size=5, stride=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "       \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "       \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "       \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "       \n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "  \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other settings: I resize pictures to 96x96. Batch size 128. I use BCELoss, Adam optimizer with learning rate=0.0002 and betas=(0.5, 0.999). After 200 epochs, we can get:\n",
    "![](fig-a.png)\n",
    "We can see: the generator learns hair and eyes well, but lack of details, and the face diversity is poor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize=128\n",
    "imageSize=96\n",
    "nz=100\n",
    "ngf=64\n",
    "ndf=64\n",
    "epoch=200\n",
    "lr=0.0002\n",
    "beta1=0.5\n",
    "data_path='train/'\n",
    "outf='imgs/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/200][140/447] Loss_D: 0.293 Loss_G 5.185\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Scale(imageSize),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(data_path, transform=transforms)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batchSize,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "netG = NetG(ngf, nz).to(device)\n",
    "netD = NetD(ndf).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "label = torch.FloatTensor(batchSize)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "for ep in range(1, epoch + 1):\n",
    "    for i, (imgs,_) in enumerate(dataloader):\n",
    "        optimizerD.zero_grad()\n",
    "        \n",
    "        imgs=imgs.to(device)\n",
    "        output = netD(imgs)\n",
    "        label.data.fill_(real_label)\n",
    "        label=label.to(device)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        \n",
    "        label.data.fill_(fake_label)\n",
    "        noise = torch.randn(batchSize, nz, 1, 1)\n",
    "        noise=noise.to(device)\n",
    "        fake = netG(noise) \n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        errD = errD_fake + errD_real\n",
    "        optimizerD.step()\n",
    "\n",
    "        optimizerG.zero_grad()\n",
    "        label.data.fill_(real_label)\n",
    "        label = label.to(device)\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        clear_output(True)\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.3f Loss_G %.3f'\n",
    "              % (ep, epoch, i, len(dataloader), errD.item(), errG.item()))\n",
    "\n",
    "    vutils.save_image(fake.data,\n",
    "                      '%s/fake_samples_epoch_%03d.png' % (outf, ep),\n",
    "                      normalize=True)\n",
    "    torch.save(netG, '%s/netG_%03d.pt' % (outf, ep))\n",
    "    torch.save(netD, '%s/netD_%03d.pt' % (outf, ep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences between DCGAN and W-GAN are:\n",
    "- For the discriminator, the sigmoid in the classifier is removed. \n",
    "- The loss functions are changed to Wasserstein Divergence. \n",
    "\n",
    "The generator is defined as:\n",
    "- Linear (100, 128)\n",
    "- Leaky ReLU\n",
    "- Linear (128, 256)\n",
    "- Batch norm \n",
    "- Leaky ReLU\n",
    "- Linear (256, 512)\n",
    "- Batch norm \n",
    "- Leaky ReLU\n",
    "- Linear (512, 1024)\n",
    "- Batch norm \n",
    "- Leaky ReLU\n",
    "- Output layer: Linear (1024, 3x64x64) + an nn.Tanh()\n",
    "\n",
    "The discriminator is defined as:\n",
    "- Linear (3x64x64, 512)\n",
    "- Leaky ReLU\n",
    "- Linear(512, 256)\n",
    "- Leaky ReLU\n",
    "- Linear(256, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (channels, img_size, img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "class WGANGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WGANGenerator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class WGANDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WGANDiscriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other settings: Batch size 64. I use self-coded Wasserstein Divergence as loss function, RMSprop optimizer with learning rate=0.00005. After 200 epochs, we can get:\n",
    "![](fig-b.png)\n",
    "We can see: Compared with DCGAN, W-GAN’s result is less distorted, but still lack of details. Maybe because the discriminator is too simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=200\n",
    "batch_size=64\n",
    "lr=0.00005\n",
    "latent_dim=100\n",
    "img_size=64\n",
    "channels=3\n",
    "n_critic=5\n",
    "clip_value=0.01\n",
    "sample_interval=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = WGANGenerator()\n",
    "discriminator = WGANDiscriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    \n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder('train/', transform=transforms)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "\n",
    "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=lr)\n",
    "optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=lr)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 199/200] [Batch 890/894] [D loss: -1.656635] [G loss: 0.977705]\n"
     ]
    }
   ],
   "source": [
    "batches_done = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        optimizer_D.zero_grad()\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "\n",
    "        fake_imgs = generator(z).detach()\n",
    "        loss_D = -torch.mean(discriminator(real_imgs)) + torch.mean(discriminator(fake_imgs))\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        for p in discriminator.parameters():\n",
    "            p.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "        if i % n_critic == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "            gen_imgs = generator(z)\n",
    "            loss_G = -torch.mean(discriminator(gen_imgs))\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            clear_output(True)\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, n_epochs, batches_done % len(dataloader), len(dataloader), loss_D.item(), loss_G.item())\n",
    "            )\n",
    "\n",
    "        if batches_done % sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], \"wgan-images/%09d.png\" % batches_done, nrow=5, normalize=True)\n",
    "            torch.save(generator, 'wgan-model/netG_%09d.pt' % (batches_done))\n",
    "            torch.save(discriminator, 'wgan-model/netD_%09d.pt' % (batches_done))\n",
    "            \n",
    "        batches_done += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda: True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print('use cuda: %s'%(use_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning based on Progressive Growing of GAN (PGGAN)\n",
    "\n",
    "As we can see, simple self-coded models perform not very well, so I decide to turn to the transfer learning. At first I wanted to try SinGAN recommended by Professor Nguyen. But after reading the paper (https://arxiv.org/abs/1905.01164), **I noticed that SinGAN works well when the pictures contain repetitive patterns (crowd of mountains, sheep, or tree branches), so it may not be good at drawing anime faces. After searching on the Internet, I select Progressive Growing of GAN (PGGAN) published by Nvidia in 2017 (https://arxiv.org/abs/1710.10196). It is also a pretty new technique.**          \n",
    "\n",
    "The basic idea of PGGAN is: learn to generate 4x4 pictures, then 8x8 pictures, then 16x16, and so on. Originally, PGGAN is used to generate 1024x1024 human faces with high resolution. I transfer it to generate 64x64 anime faces.     \n",
    "\n",
    "I use the official Pytorch API and tutorial provided by Facebook (https://pytorch.org/hub/facebookresearch_pytorch-gan-zoo_pgan/ and https://github.com/facebookresearch/pytorch_GAN_zoo, published in 2019). I deployed it on AWS server. \n",
    "![](fig-c.png)\n",
    "\n",
    "**I still keep running it before you grading, you can see it in http://18.221.62.53:8097/. Please change “main” into “mtcpgan_training”.**\n",
    "![](fig-d.png)\n",
    "The model type I choose is PGAN(progressive growing of gan).  \n",
    "![](fig-e.png)  \n",
    "\n",
    "\n",
    "Let’s see some results. I began the training on 2019-12-14. It takes over 3 days to get good 64x64 pictures.   \n",
    "\n",
    "Dec 15, 1:50 pm. This is 8x8 training (first two lines are generated pictures and the third line is the real data). Eyes are visible: \n",
    "![](fig-f.png)  \n",
    "\n",
    "Dec 15, 3:56 pm. 16x16 training (first two lines are generated pictures and the third line is the real data):\n",
    "![](fig-g.png)  \n",
    "\n",
    "Dec 16, 7:25 pm. 32x32 training (first two lines are generated pictures and the third line is the real data):\n",
    "![](fig-h.png)  \n",
    "32x32 training ran 96000 iterations.\n",
    "![](fig-i.png)  \n",
    "\n",
    "\n",
    "Dec 17, 9:10 pm. 64x64 training (first two lines are generated pictures and the third line is the real data). The result is gorgeous. Some generated faces cannot be told apart from the real data. The details are clear and rich. In this phase, we can see in the top 2 lines, the generated faces have different hair styles, hair colors, eye colors. Some girls have mouths (in Japanese cartoons, when characters close their mouths, the mouths are invisible sometimes. So it is a feature hard to learn), while there are no opened mouth in DCGAN and W-GAN generated faces. The first two girls have redness on their faces, that means they are shy. The 6, 7 and 8th girl have highlight spots in their eyes, that is the feature of liveliness in the cartoons. And the faces are in slightly different directions.    \n",
    "![](fig-j.png)  \n",
    "\n",
    "**You may think that the faces are still blurry. That is because the figure above shows the faces much larger than 64x64. If we shrink them around 64x64, the effect is marvelous (first two lines are generated pictures and the third line is the real data).**     \n",
    "![](fig-k.png)  \n",
    "\n",
    "**The 64x64 training began around 7 pm on Dec 17, and it is still running on the AWS server. When I write this report it has ran 2900 iterations only. Maybe the quality of the generates faces can be improved further later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
